<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>LBANN: Layers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">LBANN
   &#160;<span id="projectnumber">9bd98b7-ci255</span>
   </div>
   <div id="projectbrief">LivermoreBigArtificialNeuralNetworkToolkit</div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('layers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Friends</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(11)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Layers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Detailed information for layer types and options within these types can be found in this section. The layers are <a class="el" href="layers.html#learning">Learning</a>, <a class="el" href="layers.html#regularizer">Regularizer</a>, <a class="el" href="layers.html#transform">Transform</a>, <a class="el" href="layers.html#activation">Activation</a>, and <a class="el" href="layers.html#io">IO</a>.</p>
<h1><a class="anchor" id="learning"></a>
Learning</h1>
<h1><a class="anchor" id="regularizer"></a>
Regularizer</h1>
<h2><a class="anchor" id="batchNorm"></a>
Batch Normalization</h2>
<p>Batch normalization: normalize layers to zero mean/unit standard deviation. See paper: Sergey Ioffe and Christian Szegedy. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ICML 2015. This keeps a running mean and standard deviation (with exponential decay) instead of computing it over the data at test time. This approach seems to have become standard. See also: <a href="https://cthorey.github.io/backpropagation/">https://cthorey.github.io/backpropagation/</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1batch__normalization.html">lbann::batch_normalization</a></p>
<h2><a class="anchor" id="dropout"></a>
Dropout</h2>
<p>Dropout: probabilistically drop units from a layer. See this paper for full details: Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks
from overfitting." Journal of Machine Learning Research 15.1 (2014). This implementation uses the approach noted in section 10 of that paper of multiplying weights by 1/(keep probability) at training time and not modifying them at test time. The implementation recommends a keep probability of 0.5 for fully-connected layers and 0.8 for input layers as good starting points.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1dropout.html">lbann::dropout</a></p>
<h2><a class="anchor" id="selu_dropout"></a>
Selu Dropout</h2>
<p>SELU dropout: alpha-scaled dropout for use with SELU activations. See: Klambauer et al. "Self-Normalizing Neural Networks", 2017. This makes the same default assumptions as our SELU activations. The paper recommends a default dropout rate of 0.05 (keep 0.95).</p>
<p>Implementation details: <a class="el" href="classlbann_1_1selu__dropout.html">lbann::selu_dropout</a></p>
<h2><a class="anchor" id="local_response_norm_layer"></a>
Local Response Norm Layer</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1local__response__normalization__layer.html" title="Local Response Normalization layer. ">lbann::local_response_normalization_layer</a></p>
<h1><a class="anchor" id="transform"></a>
Transform</h1>
<h2><a class="anchor" id="concatenation"></a>
Concatenation</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1concatenation__layer.html" title="Concatenation layer. ">lbann::concatenation_layer</a></p>
<h2><a class="anchor" id="pooling"></a>
Pooling</h2>
<p><a class="el" href="classlbann_1_1pooling__layer.html" title="Pooling layer. ">lbann::pooling_layer</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1pooling__layer.html" title="Pooling layer. ">lbann::pooling_layer</a></p>
<h2><a class="anchor" id="slice"></a>
Slice</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1slice__layer.html" title="Slice layer. ">lbann::slice_layer</a></p>
<h2><a class="anchor" id="split"></a>
Split</h2>
<p><a class="el" href="classlbann_1_1split__layer.html" title="Split layer. ">lbann::split_layer</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1split__layer.html" title="Split layer. ">lbann::split_layer</a></p>
<h2><a class="anchor" id="sum"></a>
Sum</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1sum__layer.html" title="Sum layer. ">lbann::sum_layer</a></p>
<h1><a class="anchor" id="activation"></a>
Activation</h1>
<h2><a class="anchor" id="idlayer"></a>
Identity</h2>
<p>Identity activation function &ndash; does nothing.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1id__layer.html">lbann::id_layer</a></p>
<h2><a class="anchor" id="reluLayer"></a>
Rectified Linear Unit</h2>
<p>Rectified linear unit activation function. See: <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1relu__layer.html">lbann::relu_layer</a></p>
<h2><a class="anchor" id="leakyrelu"></a>
Leaky Relu</h2>
<p>Leaky rectified linear unit activation function. This is a ReLU variant that avoids the dying ReLU problem where a ReLU neuron can stop updating. See: Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. "Rectifier nonlinearities
improve neural network acoustic models." Proc. ICML. Vol. 30. No. 1. 2013.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1leaky__relu__layer.html">lbann::leaky_relu_layer</a>:</p>
<h2><a class="anchor" id="smoothrelu"></a>
Smooth Relu</h2>
<p>Smooth Rectified linear unit activation function. This is an approximation to the softplus.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1smooth__relu__layer.html">lbann::smooth_relu_layer</a></p>
<h2><a class="anchor" id="expLinUn"></a>
Exponential Linear Unit</h2>
<p>Exponential linear unit. Tries to speed up learning by pushing the mean of activations more towards zero by allowing negative values. Helps avoid the need for batch normalization. See: Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)" ICLR 2016.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1elu__layer.html">lbann::elu_layer</a></p>
<h2><a class="anchor" id="seluLayer"></a>
Scaled Elu</h2>
<p>SELU: scaled exponential linear unit. See: Klambauer et al. "Self-Normalizing Neural Networks", 2017. <a href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</a> By default, this assumes the goal is to normalize to 0 mean/unit variance. To accomplish this, you should also normalize input to 0 mean/unit variance (z-score), initialize with 0 mean, 1/n variance (He), and use the SELU dropout.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1selu__layer.html">lbann::selu_layer</a></p>
<h2><a class="anchor" id="sigLayer"></a>
Sigmoid</h2>
<p>Sigmoid activation function. See: <a href="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1sigmoid__layer.html">lbann::sigmoid_layer</a></p>
<h2><a class="anchor" id="softplus"></a>
Softplus</h2>
<p>Softplus activation function. This is a smooth approximation of the ReLU. See: <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1softplus__layer.html">lbann::softplus_layer</a></p>
<h2><a class="anchor" id="softmax"></a>
Softmax</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1softmax__layer.html">lbann::softmax_layer</a></p>
<h2><a class="anchor" id="tanh"></a>
Tanh</h2>
<p>Hyperbolic tangent activation function.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1tanh__layer.html">lbann::tanh_layer</a></p>
<h1><a class="anchor" id="io"></a>
IO</h1>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Tue Sep 19 2017 10:15:30 for LBANN by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.5 </li>
  </ul>
</div>
</body>
</html>
